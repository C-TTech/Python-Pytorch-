"""
--------------------------------------
Building the NN
----------------------------------------
"""
import torch
import torchvision
from torchvision import transforms, datasets
import torch.utils.data
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F

# Use out-of-sample testing data, that has never been seen by model before

# MNIST is handdrawn digits (0-9) dataset.  28x28 images.
# Downloads and stores Data as variables:
train = datasets.MNIST("", train=True, download=True,
                       transform=transforms.Compose([transforms.ToTensor()]))

test = datasets.MNIST("", train=False, download=True,
                      transform=transforms.Compose([transforms.ToTensor()]))

trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)
testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)


# -----------------------------------------------------
# Neural Network and Layers
class Net(nn.Module):

    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 64)  # (input aka 28*28, 3 layers of 64 neurons as hidden layers).  Input is into
        # the layer and output could be anything
        self.fc2 = nn.Linear(64, 64)  # Inputs are from previous layer(s)
        self.fc3 = nn.Linear(64, 64)
        self.fc4 = nn.Linear(64, 10)  # Output layer. 10 neurons because 10 classes

    # Feed forward NN.  Defining how data passes through NN.  Can make advanced models with if, else for certain layers
    # to suit application.  PYTORCH makes this simple.
    # F.relu is activation function. Rectified linear not on final layer.
    # Probability distribution required for output layer = log_softmax(x)
    # Rectified linear for optimiser also
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = self.fc4(x)
        return F.log_softmax(x, dim=1)


net = Net()
print(net)
'''
Net(
  (fc1): Linear(in_features=784, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=10, bias=True)
)
'''
# --------------------------------------------------------------

# Image must be flattened before input into NN
X = torch.rand((28, 28))
X = X.view(1, 28 * 28)
# -1 or 1 means that input is of an unknown shape.  "It's of any size"

output = net(X)
print(output)

'''
Predictions for 0,1,2,3,4,5,6,7,8,9:

tensor([[-2.3052, -2.2803, -2.3709, -2.3176, -2.3912, -2.2335, -2.1736, -2.3258,
         -2.2793, -2.3686]], grad_fn=<LogSoftmaxBackward>)

grad_fn is gradient function.  
Next, calculate how accurate the predictions are
'''
